# -*- coding: utf-8 -*-
"""Historico.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14J1Xafke1ja0hVpN_LCUQuyxGcTIFBS2

# Descripción del Proyecto

Este proyecto tiene como objetivo analizar la estacionalidad de los principales sectores del mercado financiero, identificando patrones de comportamiento, así como períodos históricamente alcistas y bajistas. A partir de estos hallazgos, se busca desarrollar un criterio de filtrado que permita priorizar las acciones más relevantes a observar durante cada período del año, con el fin de optimizar la toma de decisiones de compra y venta.

# Inicialización
"""

import yfinance as yf
import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

"""# Cargar Datos"""

# Lista de ETFs del sector como strings
etf_sector = ['XLC', 'XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLRE', 'XLK', 'XLU']

def save_sector_data_to_csv(etf_list, folder_name='etf_data'):
    # Crear carpeta si no existe
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)

    for symbol in etf_list:
        ticker = yf.Ticker(symbol)
        hist = ticker.history(period="max")
        hist.index = hist.index.date
        hist.index.name = 'date'
        file_path = os.path.join(folder_name, f"{symbol}_historical.csv")
        hist.to_csv(file_path)
        print(f"Guardado: {file_path}")

# Ejecutar función
save_sector_data_to_csv(etf_sector)

# Cargar archivos en diferentes DataFrames
df_xlc = pd.read_csv('etf_data/XLC_historical.csv', sep=',')
df_xly = pd.read_csv('etf_data/XLY_historical.csv', sep=',')
df_xlp = pd.read_csv('etf_data/XLP_historical.csv', sep=',')
df_xle = pd.read_csv('etf_data/XLE_historical.csv', sep=',')
df_xlf = pd.read_csv('etf_data/XLF_historical.csv', sep=',')
df_xlv = pd.read_csv('etf_data/XLV_historical.csv', sep=',')
df_xli = pd.read_csv('etf_data/XLI_historical.csv', sep=',')
df_xlb = pd.read_csv('etf_data/XLB_historical.csv', sep=',')
df_xlk = pd.read_csv('etf_data/XLK_historical.csv', sep=',')
df_xlre = pd.read_csv('etf_data/XLRE_historical.csv', sep=',')
df_xlu = pd.read_csv('etf_data/XLU_historical.csv', sep=',')
df_xlc.head(2)

"""# Información de los Datos"""

# Cambiar los nombres de las columnas
df_xlc.columns = df_xlc.columns.str.replace(' ', '_').str.lower()
df_xly.columns = df_xly.columns.str.replace(' ', '_').str.lower()
df_xlp.columns = df_xlp.columns.str.replace(' ', '_').str.lower()
df_xle.columns = df_xle.columns.str.replace(' ', '_').str.lower()
df_xlf.columns = df_xlf.columns.str.replace(' ', '_').str.lower()
df_xlv.columns = df_xlv.columns.str.replace(' ', '_').str.lower()
df_xli.columns = df_xli.columns.str.replace(' ', '_').str.lower()
df_xlb.columns = df_xlb.columns.str.replace(' ', '_').str.lower()
df_xlk.columns = df_xlk.columns.str.replace(' ', '_').str.lower()
df_xlre.columns = df_xlre.columns.str.replace(' ', '_').str.lower()
df_xlu.columns = df_xlu.columns.str.replace(' ', '_').str.lower()
df_xlc.tail(2)

print(df_xlc.columns)
print(df_xly.columns)
print(df_xlp.columns)
print(df_xle.columns)
print(df_xlf.columns)
print(df_xlv.columns)
print(df_xli.columns)
print(df_xlb.columns)

# Lista con los nombres de los DataFrames
dfs = [df_xlc, df_xly, df_xlp, df_xle, df_xlf, df_xlv, df_xli, df_xlb, df_xlk, df_xlre, df_xlu]

# Convertir la columna 'date' a datetime en todos los DataFrames
for df in dfs:
    df['date'] = pd.to_datetime(df['date'])

df_xly.info()

df_xly.head()

# Ver min y max de la columna 'date' en todos los DataFrames
dfs_named = {
    'df_xlc': df_xlc,
    'df_xly': df_xly,
    'df_xlp': df_xlp,
    'df_xle': df_xle,
    'df_xlf': df_xlf,
    'df_xlv': df_xlv,
    'df_xli': df_xli,
    'df_xlb': df_xlb,
    'df_xlk': df_xlk,
    'df_xlre': df_xlre,
    'df_xlu': df_xlu
}

for name, df in dfs_named.items():
    print(f"DataFrame: {name}")
    print(f"Fecha mínima: {df['date'].min()}")
    print(f"Fecha máxima: {df['date'].max()}")
    print("\n")

# Verificar si hay duplicados
for name, df in dfs_named.items():
    duplicados = df.duplicated().sum()
    print(f"DataFrame: {name} -> Duplicados: {duplicados}")

for name, df in dfs_named.items():
    print(f"\nDataFrame: {name}")
    df.info()

# Agregar columna 'quincena' a cada DataFrame
todos_limpio = []

for name, df in dfs_named.items():
    # Crear columna 'quincena'
    df['quincena'] = df['date'].apply(
        lambda x: f"{x.year}-{x.month:02d}-Q1" if x.day <= 15 else f"{x.year}-{x.month:02d}-Q2"
    )

    # Agrupar por quincena
    resumen = df.groupby('quincena').agg(
        date_inicio=('date', 'min'),
        open_inicio=('open', 'first'),
        close_fin=('close', 'last')
    ).reset_index()

    # Calcular variación
    resumen['variacion'] = (resumen['close_fin'] - resumen['open_inicio'])/resumen['open_inicio']

    # Agregar nombre del sector
    resumen['sector'] = name

    # Renombrar columnas para exportación
    resumen = resumen.rename(columns={
        'date_inicio': 'date',
        'open_inicio': 'open',
        'close_fin': 'close'
    })

    todos_limpio.append(resumen)

# Concatenar todo
df_export = pd.concat(todos_limpio, ignore_index=True)

# Exportar a CSV
df_export.to_csv("sectores_con_quincenas.csv", index=False)
print("Archivo CSV 'sectores_con_quincenas.csv' exportado con éxito.")

# Verifica si hay duplicados
df_export.duplicated(subset=['quincena', 'sector']).sum()